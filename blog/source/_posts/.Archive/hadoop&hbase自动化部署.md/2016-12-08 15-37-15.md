## Hadoop & Hbase 自动化部署
### 前言
项目组只有一台高配的服务器，故决定使用docker搭建Hadoop&Hbase等集群环境。
### docker
Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。
### 目录结构
- hadoop-hbase-hive-cluster-docker-master
    - config
    - Dockerfile
    - program
    - resize-cluster.sh
    - start-container.sh

1. config 目录存放配值文件
2. Dockerfile docker命令脚本，用于构建Dokcer镜像
3. program hadoop、hbase等安装包目录
4. resize-cluster.sh 重建镜像脚本
5. start-container.sh 启动容器脚本

### Dockerfile 说明
``` shell
FROM rastasheep/ubuntu-sshd:latest  # 基于rastasheep/ubuntu-sshd镜像

MAINTAINER zfylin   # author

WORKDIR /root #工作目录

# 配置JDK
ADD program/jdk-8u101-linux-x64.tar.gz /usr/local
RUN mv /usr/local/jdk1.8.0_101 /usr/local/jdk

# install hadoop 2.7.2
ADD program/hadoop-2.7.2.tar.gz /usr/local
RUN mv /usr/local/hadoop-2.7.2 /usr/local/hadoop

# install hbase 1.2.3 
ADD program/hbase-1.2.3-bin.tar.gz /usr/local
RUN mv /usr/local/hbase-1.2.3 /usr/local/hbase

# install hive-2.1.0
ADD program/apache-hive-2.1.0-bin.tar.gz /usr/local
RUN mv /usr/local/apache-hive-2.1.0-bin /usr/local/hive
ADD program/mysql-connector-java-5.1.40.tar.gz /tmp
RUN cp /tmp/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/local/hive/lib

# install sqoop-1.4.6
ADD program/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz /usr/local
RUN mv /usr/local/sqoop-1.4.6.bin__hadoop-2.0.4-alpha /usr/local/sqoop
RUN cp /tmp/mysql-connector-java-5.1.40/mysql-connector-java-5.1.40-bin.jar /usr/local/sqoop/lib

# set environment variable
ENV JAVA_HOME=/usr/local/jdk
ENV HADOOP_HOME=/usr/local/hadoop
ENV HBASE_HOME=/usr/local/hbase
ENV HIVE_HOME=/usr/local/hive
ENV SQOOP_HOME=/usr/local/sqoop
ENV PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/usr/local/hbase/bin:/usr/local/hive/bin:/usr/local/sqoop/bin:/usr/local/jdk/bin

# ssh without key
RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

# mkdir hadoop log
RUN  mkdir $HADOOP_HOME/logs

# copy configs
COPY config/* /tmp/
RUN mv /tmp/ssh_config ~/.ssh/config && \
    mv /tmp/hadoop-env.sh /usr/local/hadoop/etc/hadoop/hadoop-env.sh && \
    mv /tmp/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml && \
    mv /tmp/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml && \
    mv /tmp/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml && \
    mv /tmp/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml && \
    mv /tmp/slaves $HADOOP_HOME/etc/hadoop/slaves && \
    mv /tmp/start-hadoop.sh ~/start-hadoop.sh && \
    mv /tmp/run-wordcount.sh ~/run-wordcount.sh && \
    mv /tmp/hbase-env.sh $HBASE_HOME/conf/hbase-env.sh && \
    mv /tmp/hbase-site.xml $HBASE_HOME/conf/hbase-site.xml && \
    mv /tmp/regionservers $HBASE_HOME/conf/regionservers && \
    mv /tmp/start-hbase.sh ~/start-hbase.sh && \
    mv /tmp/stop-hbase.sh ~/stop-hbase.sh && \
    mv /tmp/hive-site.xml $HIVE_HOME/conf/hive-site.xml && \
    mv /tmp/hive-log4j2.properties $HIVE_HOME/conf/hive-log4j2.properties && \
    mv /tmp/hive-exec-log4j2.properties $HIVE_HOME/conf/hive-exec-log4j2.properties && \
   mv /tmp/hive-config.sh ~/hive-config.sh && \
   mv /tmp/sqoop-env.sh ${SQOOP_HOME}/conf

RUN chmod +x ~/start-hadoop.sh && \
    chmod +x ~/run-wordcount.sh && \
    chmod +x $HADOOP_HOME/sbin/start-dfs.sh && \
    chmod +x $HADOOP_HOME/sbin/start-yarn.sh && \
    chmod +x ~/start-hbase.sh && \
    chmod +x $HBASE_HOME/bin/start-hbase.sh && \
    chmod +x ~/stop-hbase.sh && \
    chmod +x ${HBASE_HOME}/bin/stop-hbase.sh && \
    chmod +x ~/hive-config.sh

CMD [ "sh", "-c", "service ssh start; bash"]

EXPOSE 22 7373 7946 9000 50010 50020 50070 50075 50090 50475 8030 8031 8032 8033 8040 8042 8060 8088 50060 2818 60000 60010
```
### resize-cluster.sh
``` shell
#!/bin/bash

# N is the node number of hadoop cluster
N=$1

if [ $# = 0 ]
then
        echo "Please specify the node number of hadoop cluster!"
        exit 1
fi

# change slaves file
i=1
rm config/slaves
rm config/regionservers
while [ $i -lt $N ]
do
        echo "hadoop-slave$i" >> config/slaves
        echo "hadoop-slave$i" >> config/regionservers
        ((i++))
done

echo ""

echo -e "\nbuild docker hadoop image\n"

# rebuild zfylin/hadoop image
sudo docker build -t zfylin/hadoop-hbase:1.0 .

echo ""
```
### start-container.sh
``` shell
#!/bin/bash

# the default node number is 3
N=${1:-3}
HADOOP_IAMGES_NAME=zfylin/hadoop-hbase:1.0
NET_NAME=none
VOLUMN_PATH=/home/zfy/data/hadoop-cluster

declare -a users
users=([1]='10.206.19.121' [2]='10.206.19.122' [3]='10.206.19.123')
h0='mirror.centos.org:10.204.76.222'
h1='hadoop-master:10.206.19.121'
h2='hadoop-slave1:10.206.19.122'
h3='hadoop-slave2:10.206.19.123'
prefix=24
via='10.206.16.11'

# start hadoop master container
sudo docker rm -f hadoop-master &> /dev/null
echo "start hadoop-master container..."
sudo docker run -itd \
                --net=${NET_NAME} \
                --privileged=true \
                --name hadoop-master \
                --hostname hadoop-master \
                --add-host="$h0" \
                --add-host="$h1" \
                --add-host="$h2" \
                --add-host="$h3" \
                -v ${VOLUMN_PATH}/hadoop-master/hdfs:/root/hdfs \
                ${HADOOP_IAMGES_NAME} &> /dev/null

echo "pipework br33 hadoop-master ${users[1]}/$prefix@$via"
pipework br33 hadoop-master ${users[1]}/$prefix@$via

# start hadoop slave container
i=1
while [ $i -lt $N ]
do
        sudo docker rm -f hadoop-slave$i &> /dev/null
        echo "start hadoop-slave$i container..."
        sudo docker run -itd \
                        --net=${NET_NAME} \
                        --privileged=true \
                        --add-host="$h0" \
                        --add-host="$h1" \
                        --add-host="$h2" \
                        --add-host="$h3" \
                        --name hadoop-slave$i \
                        --hostname hadoop-slave$i \
                        -v ${VOLUMN_PATH}/hadoop-slave$i/hdfs:/root/hdfs \
                        ${HADOOP_IAMGES_NAME} &> /dev/null
        host_name=hadoop-slave$i
        i=$(( $i + 1 ))
        echo "pipework br33 $host_name ${users[$i]}/$prefix@$via"
        pipework br33 $host_name ${users[$i]}/$prefix@$via

done

# get into hadoop master container
sudo docker exec -it hadoop-master bash
```
### 部署
- 构建Docker镜像
    sudo ./resize-cluster.sh 5
- 启动镜像
   sudo  ./start-container.sh 5
- 启动hadoop
    ./start-hadoop.sh
- 启动hbase
   ./start-hbase.sh
- 启动hive
  hiveserver2 start


